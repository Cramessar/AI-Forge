# ðŸ§  AI Forge

**AI Forge** is a locally-powered desktop AI app I built to make using models like `mistral`, `llama2`, or `codellama` easier, faster, and more funâ€”without relying on API keys or an internet connection. If you've ever thought "I just want to run a model without jumping through five cloud hoops," welcome. Youâ€™re among friends now.

This app uses [Ollama](https://ollama.com) under the hood and is fully local-first. Itâ€™s meant to be simple for general users, powerful for devs, and welcoming for anyone who wants to explore AIâ€”without a monthly bill.

---

## ðŸš€ Why I Built This

AI is powerful. But for a lot of people, itâ€™s also confusing, expensive, or locked behind services they donâ€™t control. I wanted to change that.

So I built **AI Forge** to:
- Make **local AI** more accessible for everyone
- Let folks generate, experiment, and create **without needing the cloud**
- Support amazing projects like **Ollama**, which make running models easy
- Build something the community can shape and grow together

This is a project I plan to keep working on. But honestly? The best version of AI Forge will come from more than just me. Your perspective, ideas, and skills can take this to places I havenâ€™t imagined.

Together, we can make AI tools that **belong to the people**, not the platforms. Let's bring AI to the massesâ€”no gatekeepers, no paywalls, no problem.

---

## ðŸ§© Features

- ðŸ§  Model switching via Ollama (`mistral`, `codellama`, etc.)
- ðŸ”„ Real-time streaming output (with style)
- ðŸ’¡ Syntax-highlighted code blocks (yes, finally readable code!)
- ðŸ› ï¸ CPU / GPU backend benchmarking + tuning
- ðŸ“¥ PDF, Markdown, TXT file importing
- ðŸ’¾ Automatic local history caching (via SQLite)
- ðŸ“œ Manual session save/load
- ðŸŽ¨ Dark-mode friendly with fully themed UI

Basically, itâ€™s like ChatGPT met a local dev tool, drank some Red Bull, and decided to move off-grid.

---

## ðŸ“¦ Installation

```bash
# Clone the repo
git clone https://github.com/cramessar/ai-forge.git
cd ai-forge

# Install dependencies
pip install -r requirements.txt

# Make sure Ollama is installed
# Then download a model like mistral
ollama run mistral

# Launch the app
python app.py
```

Note: Runs best on Python 3.10+ (and yes, I tested this more times than Iâ€™ve tested my own patience).

---

## ðŸ‘¤ Who This Is For

### ðŸŽ“ General Users
- Students, bloggers, storytellers, curious humans
- Want to write, summarize, or brainstorm without the tech overhead

### ðŸ§‘â€ðŸ’» Advanced Users
- Developers, researchers, tinkerers
- Want full control over models, backends, and configurations

---

## ðŸ›  How to Use It

1. **Pick a Model**
   - Use the ðŸ§  dropdown
   - I recommend `mistral` for general tasks or `codellama` for dev work

2. **Type Your Prompt**
   - Example:  
     - "Write a pitch for a cyberpunk bagel startup"  
     - "Explain quantum entanglement like Iâ€™m five"

3. **Click Generate**
   - Sit back and watch the AI do its thing

4. **Optional Tweaks**
   - Temperature = creativity
   - Max Tokens = response length

5. **Import Files**
   - PDFs, markdown, and text files are fair game

6. **Save or Load**
   - Save a session as `.json`  
   - Reopen later for ongoing projects

7. **Forget to Save?**
   - Donâ€™t worry, I added SQLite caching so your stuff sticks around  
   - (Unless you delete the file. Thenâ€¦ well, donâ€™t do that.)

---

## ðŸ§ª Performance Tweaks

- Open ðŸ› ï¸ **Settings**
- Choose **CPU**, **GPU**, or **Auto**
- Run **Benchmark** to test which is faster
- GPU's faster? Great. CPUâ€™s faster? Still great.  
- Auto just picks the winner for you, like a non-judgmental referee.

---

## ðŸ—ƒ Local History (SQLite FTW)

All prompts + responses are stored automatically in `history.db`:

- Appears in the sidebar
- Saved silently in the background
- Doesnâ€™t require saving a session manually

Think of it as a memory that doesnâ€™t forgetâ€”even when you do.

---

## âœï¸ Developer Notes

- All config lives in `config.json`
- Backend tuning and session persistence is built-in
- Database logic lives in `db.py` (SQLite by default, swappable)
- Models are served using **Ollama**â€”so get cozy with `ollama run` and `ollama list`

Pro tip: yes, you can theme it, mod it, or plug in your own models.

---

## ðŸŒ Community Call

AI Forge is open source, and Iâ€™d love your help.

- Got ideas for features?
- Want to add plugin support?
- Got a custom use case?

Whether you're writing code, spotting bugs, designing UI, or suggesting ideasâ€”youâ€™re welcome here.

Letâ€™s build something that helps people **create freely, learn deeply, and explore widely**â€”without being held hostage by cloud pricing.

---

## â¤ï¸ Thanks & Acknowledgments

- [Ollama](https://ollama.com) â€” Seriously, go support them. Theyâ€™re doing incredible work.
- Pygments â€” For making code look less like chicken scratch.
- Markdown2 â€” For doing what Markdown does best: just working.

---

## ðŸ“š Roadmap / Coming Soon

- Custom model profiles
- Queryable local history
- Plugin system (Notion? VS Code? Terminal dragons?)
- Community prompt sharing
- Batch generation

---

### ðŸ§™ Final Thoughts

> â€œNot all who wander are lost... but some are just looking for a local AI tool that respects their GPU.â€

Thanks for checking out AI Forge.  
Now go forth and generate something epic.

â€” _Chris_



## ðŸš€ V2 Improvements

Version 2 of **AI Forge** comes loaded with next-gen features designed to give both power users and tinkerers a serious upgrade in control, flexibility, and performance:

### ðŸ”— Prompt Chaining
- Chain multiple prompt templates together.
- Each response is automatically passed to the next prompt.
- Mimics a manual conversation flow with automated precision.
- Ideal for structured workflows (e.g. Q&A â†’ summary â†’ action plan).

### ðŸ§  Hugging Face Transformers Integration
- Added support for local Hugging Face models.
- No API keys or internet required â€” fully offline execution.
- Includes support for:
  - `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
  - `microsoft/phi-2`
- Easily extensible: just add to the `HF_MODEL_MAP` in `local_hf_runner.py`.

### ðŸ’¾ Model Selection Upgraded
- Unified model dropdown (Ollama + Hugging Face models).
- Seamlessly switches backend type and persists config updates.

### ðŸ“š Persistent Prompt Chaining UI
- Chain selection and ordering now persists between sessions.
- Drag-and-drop to reorder, check/uncheck to toggle chain links.

### âš™ï¸ Dynamic Performance Benchmarking
- Run CPU/GPU benchmarks to identify the fastest backend.
- Auto-adjusts backend, temperature, and max token settings.
- Saves and reuses best config.

### ðŸ–¼ï¸ Enhanced Output Rendering
- Syntax-highlighted code blocks using Pygments and HTML.
- Supports markdown with fenced code, breaks, and inline formatting.
- All output scrolls and displays beautifully in a WebEngine view.

---

> Want to contribute a new model or template? Add it to the project and update the docs â€” we welcome upgrades from fellow Forge-smiths.
 P.S the "we" is the voices in my head...just kidding its the feedback I get from my wife